{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 37px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Machine Learning<br>\n",
    "  </div> \n",
    "\n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 35px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  -<br>\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 35px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "      Illustration with Random Forests<br><br>\n",
    "  </div> \n",
    "\n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 25px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "      <font color=orange> 1 - Principles of Machine Learning </font>\n",
    "  </div>\n",
    "\n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "      BSc - Fall 2024\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principles of supervized Machine Learning\n",
    "\n",
    "-  [What is a Machine Learning model ?](#what_is_ml)<br>\n",
    "-  [Learning as empirical risk minimization](#lempirical_risk)<br>\n",
    "-  [Approximate minimizer using Stochastic Gradient Descent](#sgd)<br>\n",
    "-  [Machine Learning in practice : Linear Regression](#lr)<br>\n",
    "-  [Compute the learned parameters by hand](#compute)<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for ml\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principles of Supervized Machine Learning\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what_is_ml\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 1. What is a machine learning model ?\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "\n",
    "The situation involving the use of Machine Learning typically starts as follows : We consider a variable $y$, that we cannot directly explain or observe, but which _we assume_ to be explainable or deducible from another set of observable variables $X^1$, ..., $X^p$. We call these variables the _explanatory variables_, or _features_, or _descriptors_, whereas $y$ is usually calld the _target variable_. \n",
    "\n",
    "The target variable can be the averaged temperature of tomorrow in the city of Lyon, in which case a strong set of observable variables is the averaged temperature of the current day and of the past seven days in the same city and its neighboring cities and villages. Note that the temperature of _today_ and the temperature of _tomorrow_ are different variables: the former is always observable whatever value of \"day\", whereas the latter is never directly observable.\n",
    "\n",
    "Given a set of explanatory variables $X^1$, ..., $X^p$ and a target variable $y$, Our goal is then to _find the function_ $\\hat{y} = F(X^1, ..., X^p)$ that _best approximate the target variable_ $y$. Therefore, a Machine learning model is in its escence nothing more than a function that converts an data point $x$, described by a set of features $x^1$, ..., $x^p$, into the value $y$ of the unknown expected variable. Since there is in general no clue on how $y$ depends on the selected explanatory variables, the approach of supervized machine learning is to _learn such function based on a sample of historical data_. That is, given a set of $n$ data points $x_{1} = (x_1^{1}, ..., x^p_{1})$ along with it target value $y_{1}$,  until the data point $x_{n} = (x^1_{n}, ..., x^p_{n})$ along with it target value $y_{n}$, we wish to find the function $F$ such that $F(x^1_{i}, ..., x^p_{i})$ best approximate the value $y_{i}$ for each index $i \\leq n$.\n",
    "\n",
    "\n",
    "To summarize, what constitutes the \"supervized machine learning\" denomination are the following :\n",
    "\n",
    "- **Learning** : The model isn't a program in the usual sense but something that is build in a _data-driven_ way: We start with not a single model but a _meta-model_, an infinite collection of models, and a _learning phase_ is subsequently performed in order to pick among this bunch of models a single one, whose prediction capabilities are optimal on a certain sample of data.\n",
    "\n",
    "- **Machine** : The process of learning which model to pick-up among a meta-model is _automatic_, and based on a well-defined optimisation logic.\n",
    "\n",
    "- **Supervized** : This learning process is performed in order to best predict an _already known_ set of target values.\n",
    "\n",
    "\n",
    "The two tasks that cover a very large set of use cases in Data Science are :\n",
    "\n",
    "- Regression : The target variable $y$ is _continuous_, e.g. its values are real numbers or vectors.\n",
    "- Classification : The target variable $y$ is _categorical_, e.g. its values belong to a finite set.\n",
    "\n",
    "Some situation may involve both situations (e.g. the variable to predict if a combination of continuous and categorical variables) but in all cases the problem can be broken down into separate tasks.\n",
    "\n",
    "### 1. 什么是机器学习模型？\n",
    "\n",
    "[回到顶部](#plan)\n",
    "\n",
    "机器学习的使用场景通常从以下情况开始：我们考虑一个变量 $y$，这个变量我们无法直接解释或观察，但我们**假设**它可以通过另一组可观察变量 $X^1$, ..., $X^p$ 来解释或推导。我们称这些变量为**解释变量**、**特征**或**描述符**，而 $y$ 通常被称为**目标变量**。\n",
    "\n",
    "例如，目标变量可以是里昂市明天的平均温度，在这种情况下，一组强相关的可观察变量是该城市及其周边城市和乡村当前一天及过去七天的平均温度。注意，**今天**的温度和**明天**的温度是不同的变量：前者在任何一天的值都是可以观察到的，而后者则永远无法直接观察到。\n",
    "\n",
    "给定一组解释变量 $X^1$, ..., $X^p$ 和一个目标变量 $y$，我们的目标是**找到函数** $\\hat{y} = F(X^1, ..., X^p)$，使得该函数能**最佳地近似目标变量** $y$。因此，本质上，机器学习模型只不过是一个函数，它将由一组特征 $x^1$, ..., $x^p$ 描述的数据点 $x$ 转换为未知期望变量 $y$ 的值。由于通常没有线索表明 $y$ 如何依赖于选定的解释变量，监督式机器学习的方法是**基于一组历史数据来学习这种函数**。也就是说，给定 $n$ 个数据点 $x_{1} = (x_1^{1}, ..., x^p_{1})$ 及其目标值 $y_{1}$，直到数据点 $x_{n} = (x^1_{n}, ..., x^p_{n})$ 及其目标值 $y_{n}$，我们希望找到函数 $F$，使得 $F(x^1_{i}, ..., x^p_{i})$ 能最佳地近似每个索引 $i \\leq n$ 的值 $y_{i}$。\n",
    "\n",
    "总结来说，构成“监督式机器学习”名称的有以下几点：\n",
    "\n",
    "- **学习**：模型不是传统意义上的程序，而是以**数据驱动**的方式构建的：我们开始时并没有单一模型，而是一个**元模型**，即无穷多模型的集合，随后进行**学习阶段**，从这些模型中选择一个在某个数据样本上预测能力最优的模型。\n",
    "\n",
    "- **机器**：学习从元模型中选择模型的过程是**自动**的，并基于明确定义的优化逻辑。\n",
    "\n",
    "- **监督**：此学习过程的目标是为了最好地预测**已知**的一组目标值。\n",
    "\n",
    "数据科学中涵盖绝大多数用例的两项任务是：\n",
    "\n",
    "- 回归：目标变量 $y$ 是**连续的**，例如其值为实数或向量。\n",
    "- 分类：目标变量 $y$ 是**分类的**，例如其值属于一个有限集合。\n",
    "\n",
    "某些情况下可能涉及两种情况（例如，要预测的变量是连续变量和分类变量的组合），但在所有情况下，问题都可以分解为独立的任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"empirical_risk\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2. Learning as empirical risk minimization\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Let us describe the general formulation of the learning phase, also called fitting process, in the case of a regression problem.\n",
    "\n",
    "As said, a machine learning model is one of the infinitely many possibilities of models among a meta-model. We usually annotate a meta-model as a parameterized quantity Model(w) over an infinite set of parameters (or weights) $w \\in W = \\mathbb{R}^d$, the _learnable parameters_ or _free parameters_. The dimension $d$ is here the number of learnable parameters of the model. Then a single choise $w = (w^1, ..., w^d) \\in \\mathbb{R}^d$ gives rise to a model in the usual sense,\n",
    "\n",
    "$$model = Model(w)$$\n",
    "\n",
    "When calling a model on a data point x, the output value is denoted is generically denoted as $\\widehat{y}$, or `model.predict(x)` in order to avoid ambiguities.\n",
    "\n",
    "Since supervized learning is based on pre-existing data, let us denote by $D$ this set of data, which is a set of pairs (x, y) where x is a data point sescribed by some explanatory variables and y is the target (continuous) value that we hope to infer from x. \n",
    "\n",
    "Given a meta-model `Model` parameterized by a set $W$ of parameters and a choice $w \\in \\mathbb{R}^d$, we usually measure the _empirical error_ (or _empirical risk_, or _empirical loss_) of the model Model(\\theta) on the dataset D through the _Root Mean Squared Error_ (RMSE)\n",
    "\n",
    "$$\\text{RMSError}\\left(\\text{Model}(w), D \\right) = \\frac{1}{n} \\sqrt{\\sum_{(x, y) \\in D} \\left( \\text{Model}(w).\\text{predict}(x) - y \\right) ^2}$$\n",
    "\n",
    "Note that the empirical error of the model with parametr $w$ is 0 exactly when `Model(w).predict(x)` perfectly matches the expected value $y$, for all points of the dataset. Then _the learning phase consists in finding a solution or an approximation of the empirical risk minimisation problem_, e.g. finding a choice $\\hat{w} \\in \\mathbb{R}^d$ that is close to the optimal choice\n",
    "\n",
    "$$w^* = \\arg \\min _{w \\in \\mathbb{R}^d} \\text{RMSError}\\left(\\text{Model}(w), D \\right)$$\n",
    "\n",
    "### 2. 在经验风险最小化中学习\n",
    "\n",
    "[回到顶部](#plan)\n",
    "\n",
    "让我们以回归问题为例，描述学习阶段（也称为拟合过程）的一般公式化。\n",
    "\n",
    "如前所述，机器学习模型是在一个元模型中的无穷多种模型可能性之一。我们通常将元模型表示为一个参数化的量 `Model(w)`，其参数 $w \\in W = \\mathbb{R}^d$ 是**可学习的参数**或**自由参数**。这里的维度 $d$ 是模型的可学习参数的数量。然后，一个特定的选择 $w = (w^1, ..., w^d) \\in \\mathbb{R}^d$ 就产生了一个通常意义上的模型：\n",
    "\n",
    "$$model = Model(w)$$\n",
    "\n",
    "当在一个数据点 $x$ 上调用模型时，输出值通常表示为 $\\widehat{y}$，或者为了避免歧义，可以表示为 `model.predict(x)`。\n",
    "\n",
    "由于监督学习是基于已有数据的，因此我们用 $D$ 来表示这组数据，它是由一组对 $(x, y)$ 组成的，其中 $x$ 是由某些解释变量描述的数据点，而 $y$ 是我们希望从 $x$ 推导出的目标（连续）值。\n",
    "\n",
    "给定一个由参数集 $W$ 参数化的元模型 `Model` 以及一个选择 $w \\in \\mathbb{R}^d$，我们通常通过**均方根误差**（RMSE）来衡量模型 `Model(w)` 在数据集 $D$ 上的**经验误差**（或**经验风险**，或**经验损失**）：\n",
    "\n",
    "$$\\text{RMSError}\\left(\\text{Model}(w), D \\right) = \\frac{1}{n} \\sqrt{\\sum_{(x, y) \\in D} \\left( \\text{Model}(w).\\text{predict}(x) - y \\right) ^2}$$\n",
    "\n",
    "注意，当且仅当 `Model(w).predict(x)` 对于数据集中的所有点 $(x, y)$ 完全匹配预期值 $y$ 时，参数 $w$ 对应的模型的经验误差为0。因此，**学习阶段的目标就是寻找经验风险最小化问题的解或近似解**，即找到一个选择 $\\hat{w} \\in \\mathbb{R}^d$，使其接近于最优选择：\n",
    "\n",
    "$$w^* = \\arg \\min _{w \\in \\mathbb{R}^d} \\text{RMSError}\\left(\\text{Model}(w), D \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sgd\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 3. Approximate minimizer using Stochastic Gradient Descent\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "\n",
    "\n",
    "In practice, it is simply infeasible to test each choise and pick-up the optimal one since the parameter space $\\mathbb{R}^d$ is infinite, and it is unusual to have an expression of an exact solution to this problem. Therefore, the usual approach in order to find a choice of `Model(w)` with a decent empirical error is to iteratively compute a sequence of choice such that the empirical error decreases, until it converges towards a choice $\\hat{w}$ that is finally retained as our best. \n",
    "\n",
    "The validity of this approach is mathematically explained by the fact that the function \n",
    "\n",
    "$$E(w) := Error(Model(w), 𝐷)$$ \n",
    "\n",
    "defined over the space $W = \\mathbb{R}^d$ of learnable parameters was suitably chosen so that _it attains its infimum on a critical point_, that is, a point $w^c$ where the gradient of this function cancels,\n",
    "\n",
    "$$\\nabla E(w^c) := (\\partial_1E(w^c), ..., \\partial_dE(w^c)) = (0, ..., 0)$$\n",
    "\n",
    "The underlying idea of the sequential approach is then to build a sequence of choices $(w_{(i)})_i$ such that $\\nabla E(w_{(i)})$ decreases, and to stop the process when $\\nabla E(w_{(i)})$ is close enough to the null vector. The resulting choice of parameters is then frozen and gives rise to the model `Model(w)`, whose empirical error is then close to the lowest bound.\n",
    "\n",
    "The iterative process of chosing parameters with lower and lower gradient error is called **gradient descent**, and consists in the following : \n",
    "\n",
    " 1. We randomly initialize a choice of parameters $w_{(0)} \\in \\mathbb{R}^d$.<br>\n",
    " 2. we then compute the gradient $\\nabla E$ of the empirical error on this choice of parameters, $\\nabla E(w_{(0)})$.<br>\n",
    " 3. The next choice of parameters is then obtained by substracting _coordinate-wise_ a portion of this error on the previous choice of parameters: $w_{(1)} := w_{(0)} - \\lambda \\nabla E(w_{(0)})$.<br>\n",
    " 4. The process is repeated several times, $w_{(i+1)} := w_{(i)} - \\lambda \\nabla E(w_{(i)})$, until $\\nabla E(w_{(i)}$ satisfies a certain _stopping criterion_.<br>\n",
    " 5. The last choice of parameters is outputted in order to define the model to use.<br>\n",
    "\n",
    "The value $\\lambda > 0$ is called the _learning rate_.\n",
    "\n",
    "\n",
    "That being said, a second source of computational complexity comes from the size of the training dataset itself: computing the empirical error on a large dataset sequentialy in order to find better and better approximates of an optimal choice $w^{*}$ of parameters is infeasible in practice. Therefore, instead of computing the empirical error on the whole dataset, we compute it sample by sample. This new iterative process is called **stochastic gradient descent**, and consists in the following : \n",
    "\n",
    "1. We randomly initialize a choice of parameters $w_{(0)} \\in \\mathbb{R}^d$.<br>\n",
    "2. We randomly shuffle the dataset $D$, yielding a new fresh indexing as $(x_1, y_1), ..., (x_n, y_n)$.<br>\n",
    "3. We then consider for each index $i \\leq n$ the error function $E_i(w) := Error(Model(w), (x_i, y_i))$, and recusively define a new choice of parameters by : $w_{(i+1)} := w_{(i)} - \\lambda \\nabla E_{i+1}(w_{(i)})$<br>\n",
    "4. We repeat the process from point 2., until $\\nabla E_{i+1}(w_{(i)})$ satisfies a certain _stopping criterion_.<br>\n",
    "5. The last choice of parameters is outputted in order to define the model to use.<br>\n",
    "\n",
    "The typical values of the learning rate $\\lambda$ are 0.01, 0.001 or sometimes even less.\n",
    "\n",
    "A single pass over the full collection of data points $D$ (namely executing points 2. and 3.) is called an _epoch_. After an epoch, the choice of parameters have been updated $n$ times, so it is sometimes sufficent to perform a single epoch in order to reach the stopping criterion. Since the computational cost of computing the gradient of error $\\nabla E_{i}$ is much lower than computing $\\nabla E$, the training becomes feasible in practice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lr\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Machine Learning in practice : Linear Regression\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Let us use the most common regression model in data science : `Linear regression`. A linear regression is a machine learning model that attempts to express the target variable as a _linear combination of the explanatory variables_. More precisely, given the explanatory variables $X^1, ..., X^p$, a linear model is parameterized by $(w_0, w_1, ..., w_p) \\in \\mathbb{R}^{p+1}$ and is defined as \n",
    "\n",
    "$$\\text{Regression_model}(w_0, ..., w_p)\\text{.predict}(x_1, ..., x_p) = w_0 + \\sum_{i = 1}^p w_ix_i$$\n",
    "\n",
    "\n",
    "Let us create and fit such a model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 input variables \n",
    "# 2 data points\n",
    "\n",
    "X = [\n",
    "    [1, 2, 1],\n",
    "    [-1, 0, 0.27],\n",
    "]\n",
    "y = [\n",
    "    2, \n",
    "    2.5,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the learning phase is simply performed via the fit method embarked in the meta-model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations ! You now have a trained machine learning model that is ready to be used for inference :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.3943559633887657 + (-0.11719345122994529)*1 + (-0.11719345122994528)*2 + (-0.04277560969893002)*3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters are obtained as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w_0 = model.intercept_\n",
    "w_1, w_2, w_3 = model.coef_\n",
    "w_0, w_1, w_2, w_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this the end of the story ? well, not quite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"compute\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Compute the learned parameters by hand\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Let us consider the following set $D$ of datapoints, as explainable value/target value:\n",
    "\n",
    "$$(x_1, y_1) = (0, -1) \\qquad \\qquad \\qquad (x_2, y_2) = (1, 1)$$\n",
    "\n",
    "Our goal will be to compute the regression model parameterized by $(w_0, w_1) \\in \\mathbb{R}^2$ and defined as \n",
    "\n",
    "$$\\text{Regression_model}(w_0, w_1)\\text{.predict}(x) = w_0 + w_1x$$\n",
    "\n",
    "that minimizes the mpirical error on this dataset.\n",
    "\n",
    "#### Exercice\n",
    "\n",
    "1. Draw the dataset on a plane. Guess which choice of parameters is optimal, and provide a visual explanation.\n",
    "2. Find an explicit expression of the empirical error function $E(w_0, w_1) := RMSError(\\text{Regression_model}(w_0, w_1), D)$. Justify that minimizing $E(w_0, w_1)$ is equivalent to minimizing \n",
    "\n",
    "$$\\tilde{E}(w_0, w_1) = (w_0 + 1)^2 + (w_0 + w_1 -1)^2$$\n",
    "\n",
    "3. Compute the gradient of $\\tilde{E}$, and use it to find the global minima of this function.\n",
    "4. Compute the next 4 terms of the sequence of parameters computed by gradient descent starting from $(w_{0, (0)}, w_{0, (1)}) = (1, 1)$. Draw these points are th global minimizer on a plane.\n",
    "5. Compute the result of stochastic gradient descent on 2 epochs, where the first epoch covers the dataset in original order and the scond epoch covers the dataset in reverse order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To go further\n",
    "\n",
    "#### Variants of linear regression\n",
    "\n",
    "Different variants of linear regressions are commonly encountered in Machine Learning, namely Ridge Regression, Lasso Regression and ElasticNet. These are all regular Linear regressions in the sense that the meta-model is identical to that of regular linear regression, and the model obtained after the learning phase has an identical expression as a weighted sum of the input features. What distinguises each variant is the introduction of a _penalisation term_ in the estimation of the empirical error. Naturally, considering a different expression of the error eventually leads to a different set of optimal parameters, hence a different model. We provide the formulation of the empirical risk of the ridge variant (exact formulations may slightly vary depending on source) :\n",
    "\n",
    "$$RidgeError(Model(\\theta), D) = 1/n \\sqrt{\\sum_{(x, y) \\in D)}(Model(\\theta).predict(x) - y)^2 + \\alpha \\left(\\sum_{\\theta_i \\in \\theta} \\theta_i^2 \\right)}$$\n",
    "\n",
    "$$LassoError(Model(\\theta), D) = 1/n \\sqrt{\\sum_{(x, y) \\in D)}(Model(\\theta).predict(x) - y)^2 + \\alpha \\left(\\sum_{\\theta_i \\in \\theta} \\vert \\theta_i \\vert \\right)}$$\n",
    "\n",
    "$$ElasticError(Model(\\theta), D) = 1/n \\sqrt{\\sum_{(x, y) \\in D)}(Model(\\theta).predict(x) - y)^2 + \\alpha_1 \\left(\\sum_{\\theta_i \\in \\theta} \\theta_i^2 \\right) + \\alpha_2 \\left(\\sum_{\\theta_i \\in \\theta} \\vert \\theta_i \\vert \\right)}$$\n",
    "\n",
    "Therefore, the $L_1$ or/and $L_2$ norm of the parameter vector of the model is penalized, so that the optimal model in this scheme is the one that provides a maximum of empirical approximation while keeping reasonably-sized parameters.\n",
    "\n",
    "More example of variants are described in https://scikit-learn.org/stable/modules/linear_model.html#linear-models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bottom'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#plan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
