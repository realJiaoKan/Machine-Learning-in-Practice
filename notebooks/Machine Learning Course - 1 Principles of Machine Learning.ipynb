{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 37px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Machine Learning<br>\n",
    "  </div> \n",
    "\n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 35px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  -<br>\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 35px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "      Illustration with Random Forests<br><br>\n",
    "  </div> \n",
    "\n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 25px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "      <font color=orange> 1 - Principles of Machine Learning </font>\n",
    "  </div>\n",
    "\n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "      BSc - Fall 2024\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principles of supervized Machine Learning\n",
    "\n",
    "-  [What is a Machine Learning model ?](#what_is_ml)<br>\n",
    "-  [Learning as empirical risk minimization](#lempirical_risk)<br>\n",
    "-  [Approximate minimizer using Stochastic Gradient Descent](#sgd)<br>\n",
    "-  [Machine Learning in practice : Linear Regression](#lr)<br>\n",
    "-  [Compute the learned parameters by hand](#compute)<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for ml\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principles of Supervized Machine Learning\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what_is_ml\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 1. What is a machine learning model ?\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "\n",
    "The situation involving the use of Machine Learning typically starts as follows : We consider a variable $y$, that we cannot directly explain or observe, but which _we assume_ to be explainable or deducible from another set of observable variables $X^1$, ..., $X^p$. We call these variables the _explanatory variables_, or _features_, or _descriptors_, whereas $y$ is usually calld the _target variable_. \n",
    "\n",
    "The target variable can be the averaged temperature of tomorrow in the city of Lyon, in which case a strong set of observable variables is the averaged temperature of the current day and of the past seven days in the same city and its neighboring cities and villages. Note that the temperature of _today_ and the temperature of _tomorrow_ are different variables: the former is always observable whatever value of \"day\", whereas the latter is never directly observable.\n",
    "\n",
    "Given a set of explanatory variables $X^1$, ..., $X^p$ and a target variable $y$, Our goal is then to _find the function_ $\\hat{y} = F(X^1, ..., X^p)$ that _best approximate the target variable_ $y$. Therefore, a Machine learning model is in its escence nothing more than a function that converts an data point $x$, described by a set of features $x^1$, ..., $x^p$, into the value $y$ of the unknown expected variable. Since there is in general no clue on how $y$ depends on the selected explanatory variables, the approach of supervized machine learning is to _learn such function based on a sample of historical data_. That is, given a set of $n$ data points $x_{1} = (x_1^{1}, ..., x^p_{1})$ along with it target value $y_{1}$,  until the data point $x_{n} = (x^1_{n}, ..., x^p_{n})$ along with it target value $y_{n}$, we wish to find the function $F$ such that $F(x^1_{i}, ..., x^p_{i})$ best approximate the value $y_{i}$ for each index $i \\leq n$.\n",
    "\n",
    "\n",
    "To summarize, what constitutes the \"supervized machine learning\" denomination are the following :\n",
    "\n",
    "- **Learning** : The model isn't a program in the usual sense but something that is build in a _data-driven_ way: We start with not a single model but a _meta-model_, an infinite collection of models, and a _learning phase_ is subsequently performed in order to pick among this bunch of models a single one, whose prediction capabilities are optimal on a certain sample of data.\n",
    "\n",
    "- **Machine** : The process of learning which model to pick-up among a meta-model is _automatic_, and based on a well-defined optimisation logic.\n",
    "\n",
    "- **Supervized** : This learning process is performed in order to best predict an _already known_ set of target values.\n",
    "\n",
    "\n",
    "The two tasks that cover a very large set of use cases in Data Science are :\n",
    "\n",
    "- Regression : The target variable $y$ is _continuous_, e.g. its values are real numbers or vectors.\n",
    "- Classification : The target variable $y$ is _categorical_, e.g. its values belong to a finite set.\n",
    "\n",
    "Some situation may involve both situations (e.g. the variable to predict if a combination of continuous and categorical variables) but in all cases the problem can be broken down into separate tasks.\n",
    "\n",
    "### 1. ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Ÿ\n",
    "\n",
    "[å›åˆ°é¡¶éƒ¨](#plan)\n",
    "\n",
    "æœºå™¨å­¦ä¹ çš„ä½¿ç”¨åœºæ™¯é€šå¸¸ä»ä»¥ä¸‹æƒ…å†µå¼€å§‹ï¼šæˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªå˜é‡ $y$ï¼Œè¿™ä¸ªå˜é‡æˆ‘ä»¬æ— æ³•ç›´æ¥è§£é‡Šæˆ–è§‚å¯Ÿï¼Œä½†æˆ‘ä»¬**å‡è®¾**å®ƒå¯ä»¥é€šè¿‡å¦ä¸€ç»„å¯è§‚å¯Ÿå˜é‡ $X^1$, ..., $X^p$ æ¥è§£é‡Šæˆ–æ¨å¯¼ã€‚æˆ‘ä»¬ç§°è¿™äº›å˜é‡ä¸º**è§£é‡Šå˜é‡**ã€**ç‰¹å¾**æˆ–**æè¿°ç¬¦**ï¼Œè€Œ $y$ é€šå¸¸è¢«ç§°ä¸º**ç›®æ ‡å˜é‡**ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼Œç›®æ ‡å˜é‡å¯ä»¥æ˜¯é‡Œæ˜‚å¸‚æ˜å¤©çš„å¹³å‡æ¸©åº¦ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸€ç»„å¼ºç›¸å…³çš„å¯è§‚å¯Ÿå˜é‡æ˜¯è¯¥åŸå¸‚åŠå…¶å‘¨è¾¹åŸå¸‚å’Œä¹¡æ‘å½“å‰ä¸€å¤©åŠè¿‡å»ä¸ƒå¤©çš„å¹³å‡æ¸©åº¦ã€‚æ³¨æ„ï¼Œ**ä»Šå¤©**çš„æ¸©åº¦å’Œ**æ˜å¤©**çš„æ¸©åº¦æ˜¯ä¸åŒçš„å˜é‡ï¼šå‰è€…åœ¨ä»»ä½•ä¸€å¤©çš„å€¼éƒ½æ˜¯å¯ä»¥è§‚å¯Ÿåˆ°çš„ï¼Œè€Œåè€…åˆ™æ°¸è¿œæ— æ³•ç›´æ¥è§‚å¯Ÿåˆ°ã€‚\n",
    "\n",
    "ç»™å®šä¸€ç»„è§£é‡Šå˜é‡ $X^1$, ..., $X^p$ å’Œä¸€ä¸ªç›®æ ‡å˜é‡ $y$ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯**æ‰¾åˆ°å‡½æ•°** $\\hat{y} = F(X^1, ..., X^p)$ï¼Œä½¿å¾—è¯¥å‡½æ•°èƒ½**æœ€ä½³åœ°è¿‘ä¼¼ç›®æ ‡å˜é‡** $y$ã€‚å› æ­¤ï¼Œæœ¬è´¨ä¸Šï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹åªä¸è¿‡æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒå°†ç”±ä¸€ç»„ç‰¹å¾ $x^1$, ..., $x^p$ æè¿°çš„æ•°æ®ç‚¹ $x$ è½¬æ¢ä¸ºæœªçŸ¥æœŸæœ›å˜é‡ $y$ çš„å€¼ã€‚ç”±äºé€šå¸¸æ²¡æœ‰çº¿ç´¢è¡¨æ˜ $y$ å¦‚ä½•ä¾èµ–äºé€‰å®šçš„è§£é‡Šå˜é‡ï¼Œç›‘ç£å¼æœºå™¨å­¦ä¹ çš„æ–¹æ³•æ˜¯**åŸºäºä¸€ç»„å†å²æ•°æ®æ¥å­¦ä¹ è¿™ç§å‡½æ•°**ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç»™å®š $n$ ä¸ªæ•°æ®ç‚¹ $x_{1} = (x_1^{1}, ..., x^p_{1})$ åŠå…¶ç›®æ ‡å€¼ $y_{1}$ï¼Œç›´åˆ°æ•°æ®ç‚¹ $x_{n} = (x^1_{n}, ..., x^p_{n})$ åŠå…¶ç›®æ ‡å€¼ $y_{n}$ï¼Œæˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°å‡½æ•° $F$ï¼Œä½¿å¾— $F(x^1_{i}, ..., x^p_{i})$ èƒ½æœ€ä½³åœ°è¿‘ä¼¼æ¯ä¸ªç´¢å¼• $i \\leq n$ çš„å€¼ $y_{i}$ã€‚\n",
    "\n",
    "æ€»ç»“æ¥è¯´ï¼Œæ„æˆâ€œç›‘ç£å¼æœºå™¨å­¦ä¹ â€åç§°çš„æœ‰ä»¥ä¸‹å‡ ç‚¹ï¼š\n",
    "\n",
    "- **å­¦ä¹ **ï¼šæ¨¡å‹ä¸æ˜¯ä¼ ç»Ÿæ„ä¹‰ä¸Šçš„ç¨‹åºï¼Œè€Œæ˜¯ä»¥**æ•°æ®é©±åŠ¨**çš„æ–¹å¼æ„å»ºçš„ï¼šæˆ‘ä»¬å¼€å§‹æ—¶å¹¶æ²¡æœ‰å•ä¸€æ¨¡å‹ï¼Œè€Œæ˜¯ä¸€ä¸ª**å…ƒæ¨¡å‹**ï¼Œå³æ— ç©·å¤šæ¨¡å‹çš„é›†åˆï¼Œéšåè¿›è¡Œ**å­¦ä¹ é˜¶æ®µ**ï¼Œä»è¿™äº›æ¨¡å‹ä¸­é€‰æ‹©ä¸€ä¸ªåœ¨æŸä¸ªæ•°æ®æ ·æœ¬ä¸Šé¢„æµ‹èƒ½åŠ›æœ€ä¼˜çš„æ¨¡å‹ã€‚\n",
    "\n",
    "- **æœºå™¨**ï¼šå­¦ä¹ ä»å…ƒæ¨¡å‹ä¸­é€‰æ‹©æ¨¡å‹çš„è¿‡ç¨‹æ˜¯**è‡ªåŠ¨**çš„ï¼Œå¹¶åŸºäºæ˜ç¡®å®šä¹‰çš„ä¼˜åŒ–é€»è¾‘ã€‚\n",
    "\n",
    "- **ç›‘ç£**ï¼šæ­¤å­¦ä¹ è¿‡ç¨‹çš„ç›®æ ‡æ˜¯ä¸ºäº†æœ€å¥½åœ°é¢„æµ‹**å·²çŸ¥**çš„ä¸€ç»„ç›®æ ‡å€¼ã€‚\n",
    "\n",
    "æ•°æ®ç§‘å­¦ä¸­æ¶µç›–ç»å¤§å¤šæ•°ç”¨ä¾‹çš„ä¸¤é¡¹ä»»åŠ¡æ˜¯ï¼š\n",
    "\n",
    "- å›å½’ï¼šç›®æ ‡å˜é‡ $y$ æ˜¯**è¿ç»­çš„**ï¼Œä¾‹å¦‚å…¶å€¼ä¸ºå®æ•°æˆ–å‘é‡ã€‚\n",
    "- åˆ†ç±»ï¼šç›®æ ‡å˜é‡ $y$ æ˜¯**åˆ†ç±»çš„**ï¼Œä¾‹å¦‚å…¶å€¼å±äºä¸€ä¸ªæœ‰é™é›†åˆã€‚\n",
    "\n",
    "æŸäº›æƒ…å†µä¸‹å¯èƒ½æ¶‰åŠä¸¤ç§æƒ…å†µï¼ˆä¾‹å¦‚ï¼Œè¦é¢„æµ‹çš„å˜é‡æ˜¯è¿ç»­å˜é‡å’Œåˆ†ç±»å˜é‡çš„ç»„åˆï¼‰ï¼Œä½†åœ¨æ‰€æœ‰æƒ…å†µä¸‹ï¼Œé—®é¢˜éƒ½å¯ä»¥åˆ†è§£ä¸ºç‹¬ç«‹çš„ä»»åŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"empirical_risk\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2. Learning as empirical risk minimization\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Let us describe the general formulation of the learning phase, also called fitting process, in the case of a regression problem.\n",
    "\n",
    "As said, a machine learning model is one of the infinitely many possibilities of models among a meta-model. We usually annotate a meta-model as a parameterized quantity Model(w) over an infinite set of parameters (or weights) $w \\in W = \\mathbb{R}^d$, the _learnable parameters_ or _free parameters_. The dimension $d$ is here the number of learnable parameters of the model. Then a single choise $w = (w^1, ..., w^d) \\in \\mathbb{R}^d$ gives rise to a model in the usual sense,\n",
    "\n",
    "$$model = Model(w)$$\n",
    "\n",
    "When calling a model on a data point x, the output value is denoted is generically denoted as $\\widehat{y}$, or `model.predict(x)` in order to avoid ambiguities.\n",
    "\n",
    "Since supervized learning is based on pre-existing data, let us denote by $D$ this set of data, which is a set of pairs (x, y) where x is a data point sescribed by some explanatory variables and y is the target (continuous) value that we hope to infer from x. \n",
    "\n",
    "Given a meta-model `Model` parameterized by a set $W$ of parameters and a choice $w \\in \\mathbb{R}^d$, we usually measure the _empirical error_ (or _empirical risk_, or _empirical loss_) of the model Model(\\theta) on the dataset D through the _Root Mean Squared Error_ (RMSE)\n",
    "\n",
    "$$\\text{RMSError}\\left(\\text{Model}(w), D \\right) = \\frac{1}{n} \\sqrt{\\sum_{(x, y) \\in D} \\left( \\text{Model}(w).\\text{predict}(x) - y \\right) ^2}$$\n",
    "\n",
    "Note that the empirical error of the model with parametr $w$ is 0 exactly when `Model(w).predict(x)` perfectly matches the expected value $y$, for all points of the dataset. Then _the learning phase consists in finding a solution or an approximation of the empirical risk minimisation problem_, e.g. finding a choice $\\hat{w} \\in \\mathbb{R}^d$ that is close to the optimal choice\n",
    "\n",
    "$$w^* = \\arg \\min _{w \\in \\mathbb{R}^d} \\text{RMSError}\\left(\\text{Model}(w), D \\right)$$\n",
    "\n",
    "### 2. åœ¨ç»éªŒé£é™©æœ€å°åŒ–ä¸­å­¦ä¹ \n",
    "\n",
    "[å›åˆ°é¡¶éƒ¨](#plan)\n",
    "\n",
    "è®©æˆ‘ä»¬ä»¥å›å½’é—®é¢˜ä¸ºä¾‹ï¼Œæè¿°å­¦ä¹ é˜¶æ®µï¼ˆä¹Ÿç§°ä¸ºæ‹Ÿåˆè¿‡ç¨‹ï¼‰çš„ä¸€èˆ¬å…¬å¼åŒ–ã€‚\n",
    "\n",
    "å¦‚å‰æ‰€è¿°ï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹æ˜¯åœ¨ä¸€ä¸ªå…ƒæ¨¡å‹ä¸­çš„æ— ç©·å¤šç§æ¨¡å‹å¯èƒ½æ€§ä¹‹ä¸€ã€‚æˆ‘ä»¬é€šå¸¸å°†å…ƒæ¨¡å‹è¡¨ç¤ºä¸ºä¸€ä¸ªå‚æ•°åŒ–çš„é‡ `Model(w)`ï¼Œå…¶å‚æ•° $w \\in W = \\mathbb{R}^d$ æ˜¯**å¯å­¦ä¹ çš„å‚æ•°**æˆ–**è‡ªç”±å‚æ•°**ã€‚è¿™é‡Œçš„ç»´åº¦ $d$ æ˜¯æ¨¡å‹çš„å¯å­¦ä¹ å‚æ•°çš„æ•°é‡ã€‚ç„¶åï¼Œä¸€ä¸ªç‰¹å®šçš„é€‰æ‹© $w = (w^1, ..., w^d) \\in \\mathbb{R}^d$ å°±äº§ç”Ÿäº†ä¸€ä¸ªé€šå¸¸æ„ä¹‰ä¸Šçš„æ¨¡å‹ï¼š\n",
    "\n",
    "$$model = Model(w)$$\n",
    "\n",
    "å½“åœ¨ä¸€ä¸ªæ•°æ®ç‚¹ $x$ ä¸Šè°ƒç”¨æ¨¡å‹æ—¶ï¼Œè¾“å‡ºå€¼é€šå¸¸è¡¨ç¤ºä¸º $\\widehat{y}$ï¼Œæˆ–è€…ä¸ºäº†é¿å…æ­§ä¹‰ï¼Œå¯ä»¥è¡¨ç¤ºä¸º `model.predict(x)`ã€‚\n",
    "\n",
    "ç”±äºç›‘ç£å­¦ä¹ æ˜¯åŸºäºå·²æœ‰æ•°æ®çš„ï¼Œå› æ­¤æˆ‘ä»¬ç”¨ $D$ æ¥è¡¨ç¤ºè¿™ç»„æ•°æ®ï¼Œå®ƒæ˜¯ç”±ä¸€ç»„å¯¹ $(x, y)$ ç»„æˆçš„ï¼Œå…¶ä¸­ $x$ æ˜¯ç”±æŸäº›è§£é‡Šå˜é‡æè¿°çš„æ•°æ®ç‚¹ï¼Œè€Œ $y$ æ˜¯æˆ‘ä»¬å¸Œæœ›ä» $x$ æ¨å¯¼å‡ºçš„ç›®æ ‡ï¼ˆè¿ç»­ï¼‰å€¼ã€‚\n",
    "\n",
    "ç»™å®šä¸€ä¸ªç”±å‚æ•°é›† $W$ å‚æ•°åŒ–çš„å…ƒæ¨¡å‹ `Model` ä»¥åŠä¸€ä¸ªé€‰æ‹© $w \\in \\mathbb{R}^d$ï¼Œæˆ‘ä»¬é€šå¸¸é€šè¿‡**å‡æ–¹æ ¹è¯¯å·®**ï¼ˆRMSEï¼‰æ¥è¡¡é‡æ¨¡å‹ `Model(w)` åœ¨æ•°æ®é›† $D$ ä¸Šçš„**ç»éªŒè¯¯å·®**ï¼ˆæˆ–**ç»éªŒé£é™©**ï¼Œæˆ–**ç»éªŒæŸå¤±**ï¼‰ï¼š\n",
    "\n",
    "$$\\text{RMSError}\\left(\\text{Model}(w), D \\right) = \\frac{1}{n} \\sqrt{\\sum_{(x, y) \\in D} \\left( \\text{Model}(w).\\text{predict}(x) - y \\right) ^2}$$\n",
    "\n",
    "æ³¨æ„ï¼Œå½“ä¸”ä»…å½“ `Model(w).predict(x)` å¯¹äºæ•°æ®é›†ä¸­çš„æ‰€æœ‰ç‚¹ $(x, y)$ å®Œå…¨åŒ¹é…é¢„æœŸå€¼ $y$ æ—¶ï¼Œå‚æ•° $w$ å¯¹åº”çš„æ¨¡å‹çš„ç»éªŒè¯¯å·®ä¸º0ã€‚å› æ­¤ï¼Œ**å­¦ä¹ é˜¶æ®µçš„ç›®æ ‡å°±æ˜¯å¯»æ‰¾ç»éªŒé£é™©æœ€å°åŒ–é—®é¢˜çš„è§£æˆ–è¿‘ä¼¼è§£**ï¼Œå³æ‰¾åˆ°ä¸€ä¸ªé€‰æ‹© $\\hat{w} \\in \\mathbb{R}^d$ï¼Œä½¿å…¶æ¥è¿‘äºæœ€ä¼˜é€‰æ‹©ï¼š\n",
    "\n",
    "$$w^* = \\arg \\min _{w \\in \\mathbb{R}^d} \\text{RMSError}\\left(\\text{Model}(w), D \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sgd\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 3. Approximate minimizer using Stochastic Gradient Descent\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "\n",
    "\n",
    "In practice, it is simply infeasible to test each choise and pick-up the optimal one since the parameter space $\\mathbb{R}^d$ is infinite, and it is unusual to have an expression of an exact solution to this problem. Therefore, the usual approach in order to find a choice of `Model(w)` with a decent empirical error is to iteratively compute a sequence of choice such that the empirical error decreases, until it converges towards a choice $\\hat{w}$ that is finally retained as our best. \n",
    "\n",
    "The validity of this approach is mathematically explained by the fact that the function \n",
    "\n",
    "$$E(w) := Error(Model(w), ğ·)$$ \n",
    "\n",
    "defined over the space $W = \\mathbb{R}^d$ of learnable parameters was suitably chosen so that _it attains its infimum on a critical point_, that is, a point $w^c$ where the gradient of this function cancels,\n",
    "\n",
    "$$\\nabla E(w^c) := (\\partial_1E(w^c), ..., \\partial_dE(w^c)) = (0, ..., 0)$$\n",
    "\n",
    "The underlying idea of the sequential approach is then to build a sequence of choices $(w_{(i)})_i$ such that $\\nabla E(w_{(i)})$ decreases, and to stop the process when $\\nabla E(w_{(i)})$ is close enough to the null vector. The resulting choice of parameters is then frozen and gives rise to the model `Model(w)`, whose empirical error is then close to the lowest bound.\n",
    "\n",
    "The iterative process of chosing parameters with lower and lower gradient error is called **gradient descent**, and consists in the following : \n",
    "\n",
    " 1. We randomly initialize a choice of parameters $w_{(0)} \\in \\mathbb{R}^d$.<br>\n",
    " 2. we then compute the gradient $\\nabla E$ of the empirical error on this choice of parameters, $\\nabla E(w_{(0)})$.<br>\n",
    " 3. The next choice of parameters is then obtained by substracting _coordinate-wise_ a portion of this error on the previous choice of parameters: $w_{(1)} := w_{(0)} - \\lambda \\nabla E(w_{(0)})$.<br>\n",
    " 4. The process is repeated several times, $w_{(i+1)} := w_{(i)} - \\lambda \\nabla E(w_{(i)})$, until $\\nabla E(w_{(i)}$ satisfies a certain _stopping criterion_.<br>\n",
    " 5. The last choice of parameters is outputted in order to define the model to use.<br>\n",
    "\n",
    "The value $\\lambda > 0$ is called the _learning rate_.\n",
    "\n",
    "\n",
    "That being said, a second source of computational complexity comes from the size of the training dataset itself: computing the empirical error on a large dataset sequentialy in order to find better and better approximates of an optimal choice $w^{*}$ of parameters is infeasible in practice. Therefore, instead of computing the empirical error on the whole dataset, we compute it sample by sample. This new iterative process is called **stochastic gradient descent**, and consists in the following : \n",
    "\n",
    "1. We randomly initialize a choice of parameters $w_{(0)} \\in \\mathbb{R}^d$.<br>\n",
    "2. We randomly shuffle the dataset $D$, yielding a new fresh indexing as $(x_1, y_1), ..., (x_n, y_n)$.<br>\n",
    "3. We then consider for each index $i \\leq n$ the error function $E_i(w) := Error(Model(w), (x_i, y_i))$, and recusively define a new choice of parameters by : $w_{(i+1)} := w_{(i)} - \\lambda \\nabla E_{i+1}(w_{(i)})$<br>\n",
    "4. We repeat the process from point 2., until $\\nabla E_{i+1}(w_{(i)})$ satisfies a certain _stopping criterion_.<br>\n",
    "5. The last choice of parameters is outputted in order to define the model to use.<br>\n",
    "\n",
    "The typical values of the learning rate $\\lambda$ are 0.01, 0.001 or sometimes even less.\n",
    "\n",
    "A single pass over the full collection of data points $D$ (namely executing points 2. and 3.) is called an _epoch_. After an epoch, the choice of parameters have been updated $n$ times, so it is sometimes sufficent to perform a single epoch in order to reach the stopping criterion. Since the computational cost of computing the gradient of error $\\nabla E_{i}$ is much lower than computing $\\nabla E$, the training becomes feasible in practice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lr\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Machine Learning in practice : Linear Regression\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Let us use the most common regression model in data science : `Linear regression`. A linear regression is a machine learning model that attempts to express the target variable as a _linear combination of the explanatory variables_. More precisely, given the explanatory variables $X^1, ..., X^p$, a linear model is parameterized by $(w_0, w_1, ..., w_p) \\in \\mathbb{R}^{p+1}$ and is defined as \n",
    "\n",
    "$$\\text{Regression_model}(w_0, ..., w_p)\\text{.predict}(x_1, ..., x_p) = w_0 + \\sum_{i = 1}^p w_ix_i$$\n",
    "\n",
    "\n",
    "Let us create and fit such a model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 input variables \n",
    "# 2 data points\n",
    "\n",
    "X = [\n",
    "    [1, 2, 1],\n",
    "    [-1, 0, 0.27],\n",
    "]\n",
    "y = [\n",
    "    2, \n",
    "    2.5,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the learning phase is simply performed via the fit method embarked in the meta-model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations ! You now have a trained machine learning model that is ready to be used for inference :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.3943559633887657 + (-0.11719345122994529)*1 + (-0.11719345122994528)*2 + (-0.04277560969893002)*3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters are obtained as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w_0 = model.intercept_\n",
    "w_1, w_2, w_3 = model.coef_\n",
    "w_0, w_1, w_2, w_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this the end of the story ? well, not quite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"compute\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Compute the learned parameters by hand\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Let us consider the following set $D$ of datapoints, as explainable value/target value:\n",
    "\n",
    "$$(x_1, y_1) = (0, -1) \\qquad \\qquad \\qquad (x_2, y_2) = (1, 1)$$\n",
    "\n",
    "Our goal will be to compute the regression model parameterized by $(w_0, w_1) \\in \\mathbb{R}^2$ and defined as \n",
    "\n",
    "$$\\text{Regression_model}(w_0, w_1)\\text{.predict}(x) = w_0 + w_1x$$\n",
    "\n",
    "that minimizes the mpirical error on this dataset.\n",
    "\n",
    "#### Exercice\n",
    "\n",
    "1. Draw the dataset on a plane. Guess which choice of parameters is optimal, and provide a visual explanation.\n",
    "2. Find an explicit expression of the empirical error function $E(w_0, w_1) := RMSError(\\text{Regression_model}(w_0, w_1), D)$. Justify that minimizing $E(w_0, w_1)$ is equivalent to minimizing \n",
    "\n",
    "$$\\tilde{E}(w_0, w_1) = (w_0 + 1)^2 + (w_0 + w_1 -1)^2$$\n",
    "\n",
    "3. Compute the gradient of $\\tilde{E}$, and use it to find the global minima of this function.\n",
    "4. Compute the next 4 terms of the sequence of parameters computed by gradient descent starting from $(w_{0, (0)}, w_{0, (1)}) = (1, 1)$. Draw these points are th global minimizer on a plane.\n",
    "5. Compute the result of stochastic gradient descent on 2 epochs, where the first epoch covers the dataset in original order and the scond epoch covers the dataset in reverse order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To go further\n",
    "\n",
    "#### Variants of linear regression\n",
    "\n",
    "Different variants of linear regressions are commonly encountered in Machine Learning, namely Ridge Regression, Lasso Regression and ElasticNet. These are all regular Linear regressions in the sense that the meta-model is identical to that of regular linear regression, and the model obtained after the learning phase has an identical expression as a weighted sum of the input features. What distinguises each variant is the introduction of a _penalisation term_ in the estimation of the empirical error. Naturally, considering a different expression of the error eventually leads to a different set of optimal parameters, hence a different model. We provide the formulation of the empirical risk of the ridge variant (exact formulations may slightly vary depending on source) :\n",
    "\n",
    "$$RidgeError(Model(\\theta), D) = 1/n \\sqrt{\\sum_{(x, y) \\in D)}(Model(\\theta).predict(x) - y)^2 + \\alpha \\left(\\sum_{\\theta_i \\in \\theta} \\theta_i^2 \\right)}$$\n",
    "\n",
    "$$LassoError(Model(\\theta), D) = 1/n \\sqrt{\\sum_{(x, y) \\in D)}(Model(\\theta).predict(x) - y)^2 + \\alpha \\left(\\sum_{\\theta_i \\in \\theta} \\vert \\theta_i \\vert \\right)}$$\n",
    "\n",
    "$$ElasticError(Model(\\theta), D) = 1/n \\sqrt{\\sum_{(x, y) \\in D)}(Model(\\theta).predict(x) - y)^2 + \\alpha_1 \\left(\\sum_{\\theta_i \\in \\theta} \\theta_i^2 \\right) + \\alpha_2 \\left(\\sum_{\\theta_i \\in \\theta} \\vert \\theta_i \\vert \\right)}$$\n",
    "\n",
    "Therefore, the $L_1$ or/and $L_2$ norm of the parameter vector of the model is penalized, so that the optimal model in this scheme is the one that provides a maximum of empirical approximation while keeping reasonably-sized parameters.\n",
    "\n",
    "More example of variants are described in https://scikit-learn.org/stable/modules/linear_model.html#linear-models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bottom'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#plan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
